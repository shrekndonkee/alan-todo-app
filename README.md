ğŸ“¦ Setup
Prerequisites

Node.js (LTS)

npm

SQLite (bundled with Prisma)

Ollama installed and running

Model llama3.2 pulled in Ollama:

ollama pull llama3.2

1ï¸âƒ£ Install dependencies
npm install

2ï¸âƒ£ Environment Variables

Create a .env file in the project root (same folder as package.json) with:

# Database (Prisma uses this)
DATABASE_URL="file:./dev.db"

# Ollama AI configuration
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.2


If your class template already had other env values, keep those too.

3ï¸âƒ£ Database Setup

If your project is using Prisma migrations/seed:

npx prisma migrate dev
npx prisma db seed

â–¶ï¸ Running the App Locally

Open two terminals in the project folder.

Terminal 1 â€“ Backend (API + AI Endpoint)
npm run server


This starts the Express server on:

http://localhost:3100/api

Backend endpoints used:

GET /api/todos

GET /api/categories

POST /api/ai/help â†’ talks to Ollama

Terminal 2 â€“ Frontend (Vite Dev Server)
npm run dev


Then open the URL Vite prints (usually):

http://localhost:5173

ğŸ¤– AI Feature: â€œAI Help for a Taskâ€

This feature appears as the purple button in the Actions panel.

How it works:

Click â€œAI Help for a Taskâ€

A modal asks: â€œWhat task do you want help with?â€

You enter something like:
â€œStudy for my biology examâ€

When you submit:

Frontend sends:

POST /api/ai/help
{ "task": "Study for my biology exam" }


Backend calls your local Ollama model (llama3.2)

Ollama returns a step-by-step helpful explanation

The modal displays the guidance

The AI does NOT modify todos or the database.
It only provides helpful advice.

ğŸ›  Troubleshooting
Ollama not running / model missing

Make sure Ollama is open.
Run:

ollama pull llama3.2

AI modal says â€œAI Errorâ€

Check:

OLLAMA_BASE_URL and OLLAMA_MODEL in .env

That the backend terminal prints the request when calling /api/ai/help

That npm run server is running

API ERR_CONNECTION_REFUSED

This means the backend isnâ€™t running.

âœ”ï¸ Ensure:

npm run server


is active.

âœ”ï¸ Make sure nothing else is using port 3100.
